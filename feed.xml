<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://samvoisin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://samvoisin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-22T13:56:45+00:00</updated><id>https://samvoisin.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">R-Squared and Non-Linear Models</title><link href="https://samvoisin.github.io/blog/2024/r2-nonlinear/" rel="alternate" type="text/html" title="R-Squared and Non-Linear Models"/><published>2024-01-15T16:39:00+00:00</published><updated>2024-01-15T16:39:00+00:00</updated><id>https://samvoisin.github.io/blog/2024/r2-nonlinear</id><content type="html" xml:base="https://samvoisin.github.io/blog/2024/r2-nonlinear/"><![CDATA[<p>My <a href="https://www.samvoisin.com/blog/2023/adj-r2/">previous post</a> was about the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> and pitfalls that come with using it to compare models. However, the models we were comparing in that post were <em>linear</em> regression models. I limited the discussion to linear models for an important reason: <em>R-squared is not suitable for assessing non-linear model quality</em>.</p> <p>If you find that surprising, don’t worry - you are not alone. Using R-squared (often denoted \(R^2\)) to describe the quality of non-linear model may be the most common data science error that I see. This mistake is so common that <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892436/">entire papers</a> have been dedicated to addressing it. In this post, I am going to explain the reason why the interpretation of the \(R^2\) statistic assumes a linear model and why it does not extend to non-linear models.</p> <p>The most general definition of \(R^2\) is this:</p> \[SS_{res} = \sum_{i} (y_i - f_i)^2 = \sum_{i} e_i^2\] \[SS_{tot} = \sum_{i} (y_i - \bar{y})^2\] \[R^2 = 1 - \frac{SS_{res}}{SS_{tot}}\] <p>\(SS_{res}\) is the sum of the squares of the residuals (i.e. errors \(e_i\)) between the model (\(f_i\)) and the observed samples (\(y_i\)). This is also known as the sum of the squared errors. \(SS_{tot}\) is the total variance observed in the samples about the mean (\(\bar{y}\)). Lastly, \(\frac{SS_{res}}{SS_{tot}}\) is the ratio of residual variance to total variance observed in our dependent variable \(y\). Therefore \(R^2\) is the compliment of this ratio.</p> <p>In plain english this means that \(R^2\) is the ratio of the variance our model describes to the total variance observed in \(y\). However, there is a hidden assumption here. Lets define the variance described by our model as \(SS_{model}\). Then we can restate \(R^2\) like this:</p> \[\frac{SS_{model}}{SS_{tot}} = 1 - \frac{SS_{res}}{SS_{tot}} = \frac{SS_{tot}}{SS_{tot}} - \frac{SS_{res}}{SS_{tot}} \implies SS_{model} = SS_{tot} - SS_{res} \implies SS_{tot} = SS_{model} + SS_{res}\] <p>Thus the implication of the definition of \(R^2\) assumes that \(SS_{tot}\) is comprised of two quantities: the variance described by the model and the residual variance. The question we must now ask is whether this holds for any class of model.</p> <p>Let’s take a closer look at the total variance of the response variable \(SS_{tot}\). Recall that the total variance is composed of each observation’s squared distance from the mean of all observations \(\bar{y}\). It is here that we will introduce the predictions of a generic model \(f_i\). It is important to note that we make no assumptions about the model’s functional form.</p> \[SS_{tot} = \sum_i (y_i - \bar{y})^2 = \sum_i (y_i - f_i + f_i - \bar{y})^2 = \sum_i ((y_i - f_i) + (f_i - \bar{y}))^2\] <p>See what happened there? We introduced the quantity \(f_i\) by both adding and subtracting it inside the quadratic term. In this way we are able to incorporate the model’s predictions into our variance calculation without breaking the equality. Let’s continue by expanding the quadratic expression.</p> \[\sum_i ((y_i - f_i) + (f_i - \bar{y}))^2 = \sum_i (y_i - f_i)^2 + (f_i - \bar{y})^2 + 2(y_i - f_i)(f_i - \bar{y})\] <p>Finally, we can distribute the summation operator.</p> \[\sum_i (y_i - f_i)^2 + (f_i - \bar{y})^2 + 2(y_i - f_i)(f_i - \bar{y}) = \sum_i (y_i - f_i)^2 + \sum_i (f_i - \bar{y})^2 + \sum_i 2(y_i - f_i)(f_i - \bar{y})\] <p>We begin to see some quantities that we are already familiar with:</p> \[SS_{res} = \sum_i (y_i - f_i)^2\] \[SS_{model} = \sum_i (f_i - \bar{y})^2\] <p>But there is something else in this expression that we have not previously seen. The cross term of the quadratic \(\sum_i 2(y_i - f_i)(f_i - \bar{y})\) stands out as it did not appear when we first deconstructed \(SS_{tot}\). Why not? Let’s analyze the pieces that compose the cross term.</p> <p>Within the summation, we have a constant \(2\) that we can disregard for now. Then we have the \(i^{th}\) error \((y_i - f_i)\). The error term is multiplied with the \(i^{th}\) prediction shifted by the mean \((y_i - f_i)\). The cross term arises due to the interaction between residuals and predictions. It represents the covariance between the errors in the predictions and the (mean-centered) predictions themselves.</p> <p>To recap, the question we are trying to answer is: Why isn’t \(R^2\) suitable for assessing the fit of a non-linear model? We now have all the pieces we need to answer this question.</p> <p>We will start by examining the case when \(f\) is a least-squares linear model. It is important to remember how a model like this is fit to the dataset. I find it especially helpful to consider this optimization problem from the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Projection">perspective of projecting the dependent variable</a> \(y\) onto the space spanned by the columns of our independent variables \(X\). The linked wikipedia article provides some intuition:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/OLS_geometric_interpretation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> </div> <p>Notice how the error vector denoted by \(\hat{\epsilon}\) in the diagram is perpendicular (i.e. “orthogonal”) to the prediction vector denoted by \(X\hat{\beta}\). The error vector is the <em>rejection</em> of \(y\) projected onto the span of the columns of \(X\). From the geometric perspective, this means that the dot product of the prediction vectors and the error vectors will be zero. From the perspective of probability, this means that the predictions and their errors are independent. Formally, this means the following in the linear least squares setting:</p> \[\sum_i (y_i - f_i)(f_i - \bar{y}) = 0\] <p>And therefore</p> \[\sum_i (y_i - f_i)^2 + \sum_i (f_i - \bar{y})^2 + \sum_i 2(y_i - f_i)(f_i - \bar{y}) = SS_{res} + SS_{model} + Other = SS_{res} + SS_{model} = SS_{tot}\] <p>I once heard a mathematician describe linear/non-linear systems in the same manner that Tolstoy describes happy/unhappy families in <em>Anna Karenina</em>:</p> <blockquote> <p>Linear systems are all alike; every non-linear system is non-linear in its own way.</p> </blockquote> <p>The point is that while linear models exhibit certain properties, such as the orthogonality between predictions and errors, which simplify the interpretation of \(R^2\), non-linear models do not necessarily adhere to these properties. In particular, the assumption of independence between predictions and errors, which is crucial for the derivation of \(R^2\) as a meaningful measure of model fit, may not hold for non-linear models.</p> <p>When fitting non-linear models, the relationship between predictions and errors can be more intricate. The errors may depend on the predictions themselves, leading to non-zero covariances between errors and predictions. Consequently, the decomposition of total variance (\(SS_{tot}\)) into variance explained by the model (\(SS_{model}\)) and residual variance (\(SS_{res}\)) becomes more complex.</p> <p>The presence of non-zero covariances between errors and predictions invalidates the straightforward interpretation of \(R^2\) as the proportion of variance explained by the model. In such cases, \(R^2\) can either overestimate or underestimate the goodness of fit, leading to misleading conclusions about the model’s performance.</p> <p>To summarize, while \(R^2\) is a valuable metric for assessing model fit in the context of linear regression, its interpretation becomes problematic when applied to non-linear models due to the potential violation of assumptions regarding the independence between predictions and errors. Therefore, caution should be exercised when using \(R^2\) to evaluate non-linear models, and alternative approaches, such as <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">Akaike information criterion (AIC)</a> and/or <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">Bayesian information criterion (BIC)</a>, may be more appropriate.</p>]]></content><author><name></name></author><category term="methodology"/><category term="r-squared"/><category term="inference"/><category term="modeling"/><summary type="html"><![CDATA[The fundamental problem with using R-squared to assess non-linear model quality.]]></summary></entry><entry><title type="html">The Coefficient of Determination and Adjusted R-Squared</title><link href="https://samvoisin.github.io/blog/2023/adj-r2/" rel="alternate" type="text/html" title="The Coefficient of Determination and Adjusted R-Squared"/><published>2023-12-23T16:39:00+00:00</published><updated>2023-12-23T16:39:00+00:00</updated><id>https://samvoisin.github.io/blog/2023/adj-r2</id><content type="html" xml:base="https://samvoisin.github.io/blog/2023/adj-r2/"><![CDATA[<p>I once had the opportunity to listen to a colleague present some modeling results. While discussing their model selection procedure, my colleague compared different models using the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">R-squared statistic</a> - also known as the coefficient of determination. This often cited statistic can tell us how well a model fits a given dataset. It is interpreted as the percentage of variance in the dataset that is explained by the model. A handy tool, right? However, it comes with some caveats as we will see.</p> <p>The reason I was inspired to write this post is that, during the aforementioned engagement, the consultant stated that they chose to include a few different covariates because “the R-squared value increased when they were added to the model.” On its face that seems like a good decision. If the percentage of variance explained by the model increases with new covariates, then the model is doing a better job at explaining the relationships between covariates and the response variable, right? Unfortunately, it isn’t that simple. Let’s look at an example in code:</p> <p>For this blog post I’m using the following libraries:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
</code></pre></div></div> <p>Next we will define a target function that we want to approximate and simulate some data. Note that the first term in our function is squared. In the real world very few functions are truly linear in the predictors, but in many cases we can approximate non-linear functions with linear regression.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">target_function</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">x3</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">x1_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">x2_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">x3_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="n">noise</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="nf">target_function</span><span class="p">(</span><span class="n">x1_data</span><span class="p">,</span> <span class="n">x2_data</span><span class="p">,</span> <span class="n">x3_data</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>
</code></pre></div></div> <p>This is what the data looks like in scatter plots:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/sim_dat_plots.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> </div> <p>Now let’s fit a model and calculate the R-squared value.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x1_data</span><span class="p">,</span> <span class="n">x2_data</span><span class="p">,</span> <span class="n">x3_data</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">R^2 Score: </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">R</span><span class="o">^</span><span class="mi">2</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8728</span>
</code></pre></div></div> <p>The R-squared value produced by this code is about \(0.8728\). That means that our model is able to account for roughly \(87.28\%\) of the variance in the response variable. Because we defined the function ourselves (that is to say, the process by which our response variable is generated), we can say that this is a truthful representation of the quality of our model. But what happens if we were include an entirely unrelated covariate in our model? Intuitively, the R-squared value should not increase because we can’t explain variation in our response variable using an entirely unrelated predictor variable.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x4_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">X_prime</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">x4_data</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_prime</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_prime</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">R^2 Score: </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">R</span><span class="o">^</span><span class="mi">2</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8748</span>
</code></pre></div></div> <p>The coefficient of determination has increased. In fact, we will see an increase - or at least no decrease - with every new covariate no matter the relationship with the response variable. Surprisingly, this increase in R-squared is not due to our new covariate providing any real insight into the response variable. Rather, it is a mathematical artifact of how R-squared is calculated. The addition of any new variable, regardless of its actual relevance, can capture some additional variance by chance. This leads to an increase in the R-squared value, even though the added variable might be completely unrelated to the response variable. This phenomenon is known as “spurious correlation,” where the model appears to improve simply because it has more variables.</p> <p>We still want to compare models with different covariates. For this we can turn to the <em>adjusted</em> R-squared calculation which penalizes R-squared based on the number of covariates in the model. Here is a python function calculating adjusted R-squared.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">adjusted_r2_score</span><span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div> <p>Now let’s add several covariates one-by-one and watch what happens to the R-squared and adjusted R-squared values over time. Here is the code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_new_covariate</span><span class="p">(</span><span class="n">Xdat</span><span class="p">,</span> <span class="n">number_generator</span><span class="p">):</span>
    <span class="n">dsize</span> <span class="o">=</span> <span class="n">Xdat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">number_generator</span><span class="p">.</span><span class="nf">normal</span><span class="p">()</span>
    <span class="n">new_std</span> <span class="o">=</span> <span class="n">number_generator</span><span class="p">.</span><span class="nf">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="n">x_new</span> <span class="o">=</span> <span class="n">number_generator</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">new_mean</span><span class="p">,</span> <span class="n">new_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">dsize</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">Xdat</span><span class="p">,</span> <span class="n">x_new</span><span class="p">]</span>

<span class="n">r2_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">adj_r2_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>

    <span class="n">X</span> <span class="o">=</span> <span class="nf">get_new_covariate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">n_covariates</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
    <span class="n">adj_r2</span> <span class="o">=</span> <span class="nf">adjusted_r2_score</span><span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_covariates</span><span class="p">)</span>

    <span class="n">r2_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">r2</span><span class="p">)</span>
    <span class="n">adj_r2_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">adj_r2</span><span class="p">)</span>
</code></pre></div></div> <p>And here are plots that show the change in R-squared and adjusted R-squared values:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/r2_ar2_plots.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> </div> <p>When we run this code we see that, while R-squared is monotonically increasing (or at least staying the same), the adjusted R-squared is either lower or does not increase as much as the original R-squared upon adding an irrelevant variable. This serves as a reminder that while R-squared can be a useful metric, it should be interpreted with care, especially in models with a large number of covariates. The adjusted R-squared provides a more realistic assessment of model performance by accounting for the complexity of the model.</p> <p>The journey through R-squared and its adjusted counterpart in this post illuminates a fundamental principle in statistical modeling: simplicity and relevance are key. While R-squared can give an initial impression of a model’s explanatory power, it’s crucial to look beyond the surface. The inclusion of unnecessary covariates, as demonstrated, can artificially inflate R-squared, leading us down a misleading path of perceived accuracy. Adjusted R-squared offers a more nuanced and honest evaluation, guiding us towards models that are not just statistically sound but also meaningful and practical. As data scientists and statisticians, our aim should always be to strike a balance between model complexity and explanatory power, ensuring that our models capture the essence of the data without adding needless complexity. In the realm of data modeling, sometimes less is more.</p>]]></content><author><name></name></author><category term="methodology"/><category term="r-squared"/><category term="inference"/><category term="modeling"/><summary type="html"><![CDATA[Comparing model performance with R-squared vs Adjusted R-squared.]]></summary></entry><entry><title type="html">Topological Signal Compression</title><link href="https://samvoisin.github.io/blog/2023/tsc/" rel="alternate" type="text/html" title="Topological Signal Compression"/><published>2023-05-15T15:12:00+00:00</published><updated>2023-05-15T15:12:00+00:00</updated><id>https://samvoisin.github.io/blog/2023/tsc</id><content type="html" xml:base="https://samvoisin.github.io/blog/2023/tsc/"><![CDATA[<p>In the evolving world of Internet of Things (IoT), cheaper and more powerful devices are becoming increasingly affordable and robust. Yet, alongside these advancements are evident energy and communication limitations that restricted the devices to transmit only highly-compressed data. This challenge becomes even more pronounced in situations where devices operate outside of traditional infrastructure, relying on satellite-based communication or variable power sources such as solar energy.</p> <p>Topological Signal Compression (“TSC”) is our answer to deploying sensor networks in these restrictive conditions. TSC is a topology-based, lossy compression method that can be tailored to individual power or bandwidth needs. It allows the transmission of compressed signals that exploit the full range of a variable communications budget, ensuring optimized data transmission under any constraints.</p> <p>The algorithm works by pairing critical points in the signal modality and considering the pairing to be a simplex (this a generalization of a triangle) in a larger simplical complex. In the case of a 1-D modality these simplices are just line segments. As critical pairs are formed they are said to have a lifespan (the time between a simplex being “born” and then merged). This lifespan is called the feature’s “persistence”. Those features with greater persistence are assumed to be the most important features and are retained by TSC. The low persistence features are removed from the signal - this is the lossy part of TSC.</p> <p>I encourage you to read the full <a href="https://ieeexplore.ieee.org/abstract/document/10115654">paper</a> for more detail. It is a great application of topological data analysis! The Arxiv preprint is available <a href="https://arxiv.org/pdf/2206.07486.pdf">here</a>.</p> <p>To validate our algorithm’s capabilities, we conducted entropy calculations and ran a classification experiment on increasingly simplified signals. We drew these from the robust Free-Spoken Digit Dataset and explored the stability of our technique’s performance against conventional benchmarks. Our tests have proven successful, demonstrating that our innovative algorithm not only handles variable, challenging conditions with grace but also greatly enhances the operational efficiency of IoT devices.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/compression_ex.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An audio sample at various degrees of compression. </div>]]></content><author><name></name></author><category term="publications"/><category term="tda"/><category term="signal-processing"/><summary type="html"><![CDATA[Using the topology of a time series for lossy compression.]]></summary></entry><entry><title type="html">Using Topology to Track Ocean Patterns</title><link href="https://samvoisin.github.io/blog/2022/tda-tracking/" rel="alternate" type="text/html" title="Using Topology to Track Ocean Patterns"/><published>2022-09-27T15:12:00+00:00</published><updated>2022-09-27T15:12:00+00:00</updated><id>https://samvoisin.github.io/blog/2022/tda-tracking</id><content type="html" xml:base="https://samvoisin.github.io/blog/2022/tda-tracking/"><![CDATA[<p>As part of the <a href="https://www.darpa.mil/program/ocean-of-things">Ocean of Things</a> program organized by DARPA, my team was tasked with developing new methods for oceanographers to track relatively small - or “submesoscale” - oceanographic phenomena like eddies (i.e. vortices in the ocean). You can read the full paper <a href="https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2022GL099416">here</a>.</p> <p>This problem is challenging because large oceanographic simulations of high resolution (1-5 sq km) can contain an immense number of eddies - some of which are found inside larger eddies! Identifying these eddies isn’t too hard thanks to previous work from the oceanography community. Simultaneously, tracking each of them through time, however, was much harder.</p> <p>To solve this multi-object tracking problem, we applied tools from topological data analysis (“TDA”) to identify and track these eddies. TDA allows us to quickly identify features on scalar fields and then efficiently associate these features between time steps. Doing this for many time steps creates a set of track segments that can be chronologically ordered to determine the speed and direction of a feature between any two time steps or in aggregate over the life of that feature.</p> <p>In our case the fields are generated from large simulations of oceanic activity. We calculate a scalar measure of vorticity at all points in the simulation to get our scalar field. We identify critical points in this field as vortices and track them as they are generated and dissipate. The image below depicts a few vortices (the blue contours) and their paths (red line segments) in the Gulf of Mexico</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/tft_fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Identified and tracked submesoscale eddies </div> <p>Our research can be used for remediating pollution, predicting harmful algal blooms and informing search and rescue missions. See the paper for the full details, and feel free to reach out with any questions!</p> <p>The image below (from figure 3 in the paper) shows tracks generated in the Gulf of Mexico over three months. Can you see the large-scale current moving smaller features around the Yucatán Peninsula, Cuba and Florida?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/tft_fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Submesoscale eddy behavior by season. </div>]]></content><author><name></name></author><category term="publications"/><category term="tda"/><category term="oceanography"/><summary type="html"><![CDATA[Tracking oceanographic features with topological data analysis.]]></summary></entry></feed>