<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://samvoisin.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://samvoisin.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-01-03T13:29:20-05:00</updated><id>https://samvoisin.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">The Coefficient of Determination and Adjusted R-Squared</title><link href="https://samvoisin.github.io/blog/2023/adj-r2/" rel="alternate" type="text/html" title="The Coefficient of Determination and Adjusted R-Squared" /><published>2023-12-23T11:39:00-05:00</published><updated>2023-12-23T11:39:00-05:00</updated><id>https://samvoisin.github.io/blog/2023/adj-r2</id><content type="html" xml:base="https://samvoisin.github.io/blog/2023/adj-r2/"><![CDATA[<p>On a recent client engagement, I had the opportunity to listen to another consultant present some modeling results. While presenting the model, the consultant compared different models using the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">R-squared statistic</a> - also known as the coefficient of determination. This often cited statistic can tell us how well a model fits a given dataset. It is interpreted as the percentage of variance in the dataset that is explained by the model. A handy tool, right? However, it comes with some caveats as we will see.</p>

<p>The reason I was inspired to write this post is that, during the aforementioned engagement, the consultant stated that they chose to include a few different covariates because “the R-squared value increased when they were added to the model.” On its face that seems like a good decision. If the percentage of variance explained by the model increases with new covariates, then the model is doing a better job at explaining the relationships between covariates and the response variable, right? Unfortunately, it isn’t that simple. Let’s look at an example in code:</p>

<p>For this blog post I’m using the following libraries:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
</code></pre></div></div> <p>Next we will define a target function that we want to approximate and simulate some data. Note that the first term in our function is squared. In the real world very few functions are truly linear in the predictors, but in many cases we can approximate non-linear functions with linear regression.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">target_function</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">x3</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">x1_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">x2_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">x3_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="n">noise</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="nf">target_function</span><span class="p">(</span><span class="n">x1_data</span><span class="p">,</span> <span class="n">x2_data</span><span class="p">,</span> <span class="n">x3_data</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>
</code></pre></div></div> <p>This is what the data looks like in scatter plots:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/sim_dat_plots.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> </div> <p>Now let’s fit a model and calculate the R-squared value.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x1_data</span><span class="p">,</span> <span class="n">x2_data</span><span class="p">,</span> <span class="n">x3_data</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">R^2 Score: </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">R</span><span class="o">^</span><span class="mi">2</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8728</span>
</code></pre></div></div> <p>The R-squared value produced by this code is about \(0.8728\). That means that our model is able to account for roughly \(87.28\%\) of the variance in the response variable. Because we defined the function ourselves (that is to say, the process by which our response variable is generated), we can say that this is a truthful representation of the quality of our model. But what happens if we were include an entirely unrelated covariate in our model? Intuitively, the R-squared value should not increase because we can’t explain variation in our response variable using an entirely unrelated predictor variable.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x4_data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">X_prime</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">x4_data</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_prime</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_prime</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">R^2 Score: </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">R</span><span class="o">^</span><span class="mi">2</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8748</span>
</code></pre></div></div> <p>The coefficient of determination has increased. In fact, we will see an increase - or at least no decrease - with every new covariate no matter the relationship with the response variable. Surprisingly, this increase in R-squared is not due to our new covariate providing any real insight into the response variable. Rather, it is a mathematical artifact of how R-squared is calculated. The addition of any new variable, regardless of its actual relevance, can capture some additional variance by chance. This leads to an increase in the R-squared value, even though the added variable might be completely unrelated to the response variable. This phenomenon is known as “spurious correlation,” where the model appears to improve simply because it has more variables.</p> <p>We still want to compare models with different covariates. For this we can turn to the <em>adjusted</em> R-squared calculation which penalizes R-squared based on the number of covariates in the model. Here is a python function calculating adjusted R-squared.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">adjusted_r2_score</span><span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div> <p>Now let’s add several covariates one-by-one and watch what happens to the R-squared and adjusted R-squared values over time. Here is the code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_new_covariate</span><span class="p">(</span><span class="n">Xdat</span><span class="p">,</span> <span class="n">number_generator</span><span class="p">):</span>
    <span class="n">dsize</span> <span class="o">=</span> <span class="n">Xdat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">number_generator</span><span class="p">.</span><span class="nf">normal</span><span class="p">()</span>
    <span class="n">new_std</span> <span class="o">=</span> <span class="n">number_generator</span><span class="p">.</span><span class="nf">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="n">x_new</span> <span class="o">=</span> <span class="n">number_generator</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">new_mean</span><span class="p">,</span> <span class="n">new_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">dsize</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">Xdat</span><span class="p">,</span> <span class="n">x_new</span><span class="p">]</span>

<span class="n">r2_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">adj_r2_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>

    <span class="n">X</span> <span class="o">=</span> <span class="nf">get_new_covariate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">n_covariates</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
    <span class="n">adj_r2</span> <span class="o">=</span> <span class="nf">adjusted_r2_score</span><span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_covariates</span><span class="p">)</span>

    <span class="n">r2_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">r2</span><span class="p">)</span>
    <span class="n">adj_r2_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">adj_r2</span><span class="p">)</span>
</code></pre></div></div> <p>And here are plots that show the change in R-squared and adjusted R-squared values:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/r2_ar2_plots.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> </div> <p>When we run this code we see that, while R-squared is monotonically increasing (or at least staying the same), the adjusted R-squared is either lower or does not increase as much as the original R-squared upon adding an irrelevant variable. This serves as a reminder that while R-squared can be a useful metric, it should be interpreted with care, especially in models with a large number of covariates. The adjusted R-squared provides a more realistic assessment of model performance by accounting for the complexity of the model.</p> <p>The journey through R-squared and its adjusted counterpart in this post illuminates a fundamental principle in statistical modeling: simplicity and relevance are key. While R-squared can give an initial impression of a model’s explanatory power, it’s crucial to look beyond the surface. The inclusion of unnecessary covariates, as demonstrated, can artificially inflate R-squared, leading us down a misleading path of perceived accuracy. Adjusted R-squared offers a more nuanced and honest evaluation, guiding us towards models that are not just statistically sound but also meaningful and practical. As data scientists and statisticians, our aim should always be to strike a balance between model complexity and explanatory power, ensuring that our models capture the essence of the data without adding needless complexity. In the realm of data modeling, sometimes less is more.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Comparing model performance with R-squared vs Adjusted R-squared.]]></summary></entry><entry><title type="html">Topological Signal Compression</title><link href="https://samvoisin.github.io/blog/2023/tsc/" rel="alternate" type="text/html" title="Topological Signal Compression"/><published>2023-05-15T11:12:00-04:00</published><updated>2023-05-15T11:12:00-04:00</updated><id>https://samvoisin.github.io/blog/2023/tsc</id><content type="html" xml:base="https://samvoisin.github.io/blog/2023/tsc/"><![CDATA[<p>In the evolving world of Internet of Things (IoT), cheaper and more powerful devices are becoming increasingly affordable and robust. Yet, alongside these advancements are evident energy and communication limitations that restricted the devices to transmit only highly-compressed data. This challenge becomes even more pronounced in situations where devices operate outside of traditional infrastructure, relying on satellite-based communication or variable power sources such as solar energy.</p> <p>Topological Signal Compression (“TSC”) is our answer to deploying sensor networks in these restrictive conditions. TSC is a topology-based, lossy compression method that can be tailored to individual power or bandwidth needs. It allows the transmission of compressed signals that exploit the full range of a variable communications budget, ensuring optimized data transmission under any constraints.</p> <p>The algorithm works by pairing critical points in the signal modality and considering the pairing to be a simplex (this a generalization of a triangle) in a larger simplical complex. In the case of a 1-D modality these simplices are just line segments. As critical pairs are formed they are said to have a lifespan (the time between a simplex being “born” and then merged). This lifespan is called the feature’s “persistence”. Those features with greater persistence are assumed to be the most important features and are retained by TSC. The low persistence features are removed from the signal - this is the lossy part of TSC.</p> <p>I encourage you to read the full <a href="https://ieeexplore.ieee.org/abstract/document/10115654">paper</a> for more detail. It is a great application of topological data analysis! The Arxiv preprint is available <a href="https://arxiv.org/pdf/2206.07486.pdf">here</a>.</p> <p>To validate our algorithm’s capabilities, we conducted entropy calculations and ran a classification experiment on increasingly simplified signals. We drew these from the robust Free-Spoken Digit Dataset and explored the stability of our technique’s performance against conventional benchmarks. Our tests have proven successful, demonstrating that our innovative algorithm not only handles variable, challenging conditions with grace but also greatly enhances the operational efficiency of IoT devices.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/compression_ex.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An audio sample at various degrees of compression. </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Using the topology of a time series for lossy compression.]]></summary></entry><entry><title type="html">Using Topology to Track Ocean Patterns</title><link href="https://samvoisin.github.io/blog/2022/tda-tracking/" rel="alternate" type="text/html" title="Using Topology to Track Ocean Patterns"/><published>2022-09-27T11:12:00-04:00</published><updated>2022-09-27T11:12:00-04:00</updated><id>https://samvoisin.github.io/blog/2022/tda-tracking</id><content type="html" xml:base="https://samvoisin.github.io/blog/2022/tda-tracking/"><![CDATA[<p>As part of the <a href="https://www.darpa.mil/program/ocean-of-things">Ocean of Things</a> program organized by DARPA, my team was tasked with developing new methods for oceanographers to track relatively small - or “submesoscale” - oceanographic phenomena like eddies (i.e. vortices in the ocean). You can read the full paper <a href="https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2022GL099416">here</a>.</p> <p>This problem is challenging because large oceanographic simulations of high resolution (1-5 sq km) can contain an immense number of eddies - some of which are found inside larger eddies! Identifying these eddies isn’t too hard thanks to previous work from the oceanography community. Simultaneously, tracking each of them through time, however, was much harder.</p> <p>To solve this multi-object tracking problem, we applied tools from topological data analysis (“TDA”) to identify and track these eddies. TDA allows us to quickly identify features on scalar fields and then efficiently associate these features between time steps. Doing this for many time steps creates a set of track segments that can be chronologically ordered to determine the speed and direction of a feature between any two time steps or in aggregate over the life of that feature.</p> <p>In our case the fields are generated from large simulations of oceanic activity. We calculate a scalar measure of vorticity at all points in the simulation to get our scalar field. We identify critical points in this field as vortices and track them as they are generated and dissipate. The image below depicts a few vortices (the blue contours) and their paths (red line segments) in the Gulf of Mexico</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/tft_fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Identified and tracked submesoscale eddies </div> <p>Our research can be used for remediating pollution, predicting harmful algal blooms and informing search and rescue missions. See the paper for the full details, and feel free to reach out with any questions!</p> <p>The image below (from figure 3 in the paper) shows tracks generated in the Gulf of Mexico over three months. Can you see the large-scale current moving smaller features around the Yucatán Peninsula, Cuba and Florida?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/tft_fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Submesoscale eddy behavior by season. </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Tracking oceanographic features with topological data analysis.]]></summary></entry></feed>
